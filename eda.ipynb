{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(img_name_list: list , path: os.path):\n",
    "    \"\"\"\n",
    "    Combines the complete path to each of the images in a image folder\n",
    "    \n",
    "    \"\"\"\n",
    "    complete_img_name_list = []\n",
    "    for i in tqdm(range(len(img_name_list))):\n",
    "        complete_img_name = os.path.join(path , img_name_list[i])\n",
    "        # complete_img_name_list.append(complete_img_name)\n",
    "        img_name_list[i] = complete_img_name\n",
    "\n",
    "        # print(complete_img_name)\n",
    "    # return complete_img_name_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35126/35126 [00:00<00:00, 2322191.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/eyepacs/train/train/17023_right.jpeg', 'data/eyepacs/train/train/32675_left.jpeg', 'data/eyepacs/train/train/32835_right.jpeg', 'data/eyepacs/train/train/24615_left.jpeg', 'data/eyepacs/train/train/16752_right.jpeg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path=\"data/eyepacs\"\n",
    "subset = \"train\"\n",
    "\n",
    "subset_dir = (os.path.join(dataset_path , subset))\n",
    "        # image dir for that particular subset\n",
    "img_dir = os.path.join(subset_dir , subset)\n",
    "# containes all the images name\n",
    "img_names = os.listdir(img_dir)\n",
    "add_path(img_name_list=img_names , path=img_dir)\n",
    "\n",
    "print(img_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dir = os.path.join(dataset_path , subset)\n",
    "subset_files = os.listdir(subset_dir)\n",
    "label_csv_path = None\n",
    "for file in subset_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        label_csv_path = os.path.join(subset_dir , file)\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(label_csv_path)\n",
    "label_dic = {img_name : label for img_name , label in zip(labels_df['image']  , labels_df['level'])} \n",
    "#get original image\n",
    "label_inorder = []\n",
    "if subset == \"train\":\n",
    "    for img_name in img_names:\n",
    "        label_inorder.append(label_dic[img_name.split('/')[-1].rstrip('.jpeg')])\n",
    "\n",
    "if subset == \"test\":\n",
    "    for img_name in img_names:\n",
    "        label_inorder.append(label_dic[img_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "data/eyepacs/train/train/26581_left.jpeg\n"
     ]
    }
   ],
   "source": [
    "print(label_inorder[-1284])\n",
    "print(img_names[-1284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17023_right\n"
     ]
    }
   ],
   "source": [
    "s = \"data/eyepacs/train/train/17023_right.jpeg\"\n",
    "\n",
    "print(s.split('/')[-1].rstrip('.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 7, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2,3,4]\n",
    "l2 = [5,7,3,2]\n",
    "l3 = []\n",
    "l3.extend(l1)\n",
    "l3.extend(l2)\n",
    "\n",
    "print(l3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryClassificationMetrics(loader: DataLoader, model: nn.Module) -> dict:\n",
    "    \"\"\"\n",
    "    Test your Binary classification model against comprehensive list of metrics.\n",
    "    \n",
    "    Args:\n",
    "        loader (DataLoader): Test DataLoader\n",
    "        model (nn.Module): Model to be tested\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize counters\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    \n",
    "    # For ROC calculation\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            \n",
    "            out = model(img)\n",
    "            probabilities = torch.softmax(out, dim=1)\n",
    "            _, pred = out.max(1)\n",
    "            \n",
    "            # Store probabilities and labels for ROC calculation\n",
    "            all_probs.append(probabilities[:, 1].cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "            \n",
    "            # Basic counts\n",
    "            true_positive += ((pred == 1) & (label == 1)).sum().item()\n",
    "            false_positive += ((pred == 1) & (label == 0)).sum().item()\n",
    "            false_negative += ((pred == 0) & (label == 1)).sum().item()\n",
    "            true_negative += ((pred == 0) & (label == 0)).sum().item()\n",
    "    \n",
    "    # Calculate total samples\n",
    "    num_samples = true_positive + true_negative + false_positive + false_negative\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = ((true_positive + true_negative) / num_samples) * 100\n",
    "    \n",
    "    # Handle division by zero cases\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) > 0 else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    numerator = (true_positive * true_negative) - (false_positive * false_negative)\n",
    "    denominator = sqrt((true_positive + false_positive) * \n",
    "                      (true_positive + false_negative) * \n",
    "                      (true_negative + false_positive) * \n",
    "                      (true_negative + false_negative))\n",
    "    mcc = numerator / denominator if denominator > 0 else 0\n",
    "    \n",
    "    # Kappa Coefficient\n",
    "    observed_accuracy = (true_positive + true_negative) / num_samples\n",
    "    expected_accuracy = (((true_positive + false_positive) * \n",
    "                         (true_positive + false_negative)) +\n",
    "                        ((false_negative + true_negative) * \n",
    "                         (false_positive + true_negative))) / (num_samples ** 2)\n",
    "    kappa = (observed_accuracy - expected_accuracy) / (1 - expected_accuracy) if expected_accuracy != 1 else 0\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        all_probs = torch.cat(all_probs, dim=0).numpy()\n",
    "        all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Return all metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1_score,\n",
    "        'mcc': mcc,\n",
    "        'kappa': kappa,\n",
    "        'roc_auc': roc_auc,\n",
    "        # Additional useful metrics\n",
    "        'confusion_matrix': {\n",
    "            'true_positive': true_positive,\n",
    "            'false_positive': false_positive,\n",
    "            'false_negative': false_negative,\n",
    "            'true_negative': true_negative\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MultiCrop(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self , encoder , head):\n",
    "        super(MultiCrop , self).__init__()\n",
    "        encoder.fc = nn.Identity()\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self , x):\n",
    "        crops = torch.cumsum(torch.unique_consecutive(\n",
    "            torch.tensor([inp.shape[-1] for inp in x]),\n",
    "            return_counts=True\n",
    "\n",
    "        )[1] , 0)\n",
    "\n",
    "        start_index = 0\n",
    "\n",
    "        for end_index in crops:\n",
    "            _out = self.backbone(torch.cat(x[start_index : end_index]))\n",
    "            if start_index == 0:\n",
    "                output = _out\n",
    "            \n",
    "            else:\n",
    "                output =  torch.cat((output , _out))\n",
    "\n",
    "            start_index = end_index\n",
    "\n",
    "        output = self.head(output)\n",
    "        return output\n",
    "    \n",
    "class DINOHead(nn.Module):\n",
    "\n",
    "    def __init__(self , in_dim , out_dim , hidden_dim=2048 , bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim , hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim , hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim , bottleneck_dim),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.last_later = nn.Linear(bottleneck_dim , out_dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self , m):\n",
    "        if isinstance(m , nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight , std=0.02)\n",
    "\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias , 0)\n",
    "\n",
    "    def forward(self , x):\n",
    "        x = self.mlp(x)\n",
    "        x = F.normalize(x , dim=-1)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DINO_loss(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim , teach_temp=0.04 , student_temp=0.1 , center_mom=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.teach_temp = teach_temp\n",
    "        self.center_mom = center_mom\n",
    "\n",
    "        self.register_buffer(\"center\" , torch.zeros(1, out_dim))\n",
    "\n",
    "    def forward(self , student_out , teacher_out):\n",
    "        student_out = student_out / self.student_temp\n",
    "        teacher_out = teacher_out / self.teach_temp\n",
    "\n",
    "        student_probs = F.softmax(student_out ,dim=-1)\n",
    "        teacher_probs = F.softmax((teacher_out - self.center) ,dim=-1).detach()\n",
    "\n",
    "        loss = torch.sum(-teacher_probs * torch.log_softmax(student_out , dim=-1) , dim=-1).mean()\n",
    "\n",
    "        self.update_center(teacher_out)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self , teacher_output):\n",
    "\n",
    "        batch_center = torch.sum(teacher_output , dim=0 , keepdim=True)\n",
    "        batch_center = batch_center / len(teacher_output)\n",
    "\n",
    "        self.center = self.center * self.center_mom + batch_center * (1 - self.center_mom)\n",
    "\n",
    "class DINO(nn.Module):\n",
    "\n",
    "    def __init__(self , student , teacher , out_dim , teacher_temp=0.04 , student_temp=0.1 , center_mom=0.9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.student = MultiCrop(\n",
    "            student , \n",
    "            DINOHead(student.num_features , out_dim)\n",
    "        )\n",
    "        self.teacher = MultiCrop(\n",
    "            teacher,\n",
    "            DINOHead(teacher.num_features , out_dim)\n",
    "        )\n",
    "\n",
    "        # no gradients for teacher \n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "        self.loss_fn = DINO_loss(\n",
    "            out_dim=out_dim,\n",
    "            teacher_temp=teacher_temp,\n",
    "            student_temp=student_temp,\n",
    "            center_mom=center_mom,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_teacher(self , m=0.996):\n",
    "        for param_q , param_k in zip(self.student.parameters() , self.teacher.parameters()):\n",
    "            param_k.data.mul_(m).add_((1-m) * param_q.detach().data)\n",
    "\n",
    "    def forward(self , x):\n",
    "\n",
    "        student_output = self.student(x)\n",
    "        teacher_output = self.teacher(x[:2]) # only the larger global views\n",
    "        loss = self.loss_fn(student_output , teacher_output)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def dino_transforms():\n",
    "    global_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    local_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(96),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    return global_transform, local_transform\n",
    "\n",
    "\n",
    "def train_one_epoch(model , data_loader , optimizer , device , epoch):\n",
    "\n",
    "    model.train()\n",
    "    total_loss= 0\n",
    "    for batch_idx , img in enumerate(data_loader):\n",
    "\n",
    "        img = [im.to(device) for im in img]\n",
    "\n",
    "        loss = model(img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.update_teacher()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"epoch : {epoch} , batch : {batch_idx} , loss : {loss.item():.2f}\")\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "         \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RetinalAugmentationPipeline:\n",
    "    def __init__(self, \n",
    "                 image_size=256,\n",
    "                 global_crop_size=(224, 224),\n",
    "                 local_crop_size=(96, 96),\n",
    "                 num_global_crops=2,\n",
    "                 num_local_crops=8):\n",
    "        \n",
    "        self.num_global_crops = num_global_crops\n",
    "        self.num_local_crops = num_local_crops\n",
    "        \n",
    "        # Basic preprocessing for all views\n",
    "        self.basic_transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),  # Enhance contrast\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # Global view transformations (stronger)\n",
    "        self.global_transform = A.Compose([\n",
    "            A.RandomResizedCrop(height=global_crop_size[0], \n",
    "                              width=global_crop_size[1],\n",
    "                              scale=(0.4, 1.0)),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            # Color augmentations\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.2,\n",
    "                    contrast_limit=0.2,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.ColorJitter(\n",
    "                    brightness=0.2,\n",
    "                    contrast=0.2,\n",
    "                    saturation=0.2,\n",
    "                    hue=0.1,\n",
    "                    p=1.0\n",
    "                ),\n",
    "            ], p=0.8),\n",
    "            # Blur and noise\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "            ], p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # Local view transformations (weaker)\n",
    "        self.local_transform = A.Compose([\n",
    "            A.RandomResizedCrop(height=local_crop_size[0], \n",
    "                              width=local_crop_size[1],\n",
    "                              scale=(0.05, 0.4)),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            # Lighter color augmentations\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1,\n",
    "                contrast_limit=0.1,\n",
    "                p=0.7\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def preserve_circular_fov(self, image):\n",
    "        \"\"\"Preserve circular field of view in fundus images\"\"\"\n",
    "        height, width = image.shape[:2]\n",
    "        circle_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        center = (width // 2, height // 2)\n",
    "        radius = min(width, height) // 2\n",
    "        cv2.circle(circle_mask, center, radius, 1, -1)\n",
    "        \n",
    "        # Apply mask to each channel\n",
    "        if len(image.shape) == 3:\n",
    "            circle_mask = np.stack([circle_mask] * image.shape[2], axis=-1)\n",
    "        \n",
    "        return image * circle_mask\n",
    "\n",
    "    def vessel_enhancement(self, image):\n",
    "        \"\"\"Enhance vessel structures using CLAHE\"\"\"\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Apply CLAHE to luminance channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l = clahe.apply(l)\n",
    "        \n",
    "        # Merge channels back\n",
    "        lab = cv2.merge((l, a, b))\n",
    "        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        return enhanced\n",
    "\n",
    "    def __call__(self, image, region_masks=None):\n",
    "        \"\"\"\n",
    "        Apply augmentations to create multiple views of the image\n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            region_masks: anatomical region masks (optional)\n",
    "        Returns:\n",
    "            list of augmented views, transformed region masks\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to numpy if needed\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "\n",
    "        # Basic preprocessing\n",
    "        preprocessed = self.basic_transform(image=image)['image']\n",
    "        \n",
    "        # Preserve circular FOV\n",
    "        preprocessed = self.preserve_circular_fov(preprocessed)\n",
    "        \n",
    "        # Enhance vessel structures\n",
    "        preprocessed = self.vessel_enhancement(preprocessed)\n",
    "        \n",
    "        views = []\n",
    "        transformed_masks = []\n",
    "        \n",
    "        # Create global views\n",
    "        for _ in range(self.num_global_crops):\n",
    "            aug = self.global_transform(image=preprocessed)\n",
    "            views.append(aug['image'])\n",
    "            \n",
    "            if region_masks is not None:\n",
    "                # Apply same transformation to masks\n",
    "                aug_mask = self.global_transform(image=region_masks)['image']\n",
    "                transformed_masks.append(aug_mask)\n",
    "        \n",
    "        # Create local views\n",
    "        for _ in range(self.num_local_crops):\n",
    "            aug = self.local_transform(image=preprocessed)\n",
    "            views.append(aug['image'])\n",
    "            \n",
    "            if region_masks is not None:\n",
    "                aug_mask = self.local_transform(image=region_masks)['image']\n",
    "                transformed_masks.append(aug_mask)\n",
    "        \n",
    "        return views, transformed_masks if region_masks is not None else views\n",
    "\n",
    "# Example usage with dataloader\n",
    "class RetinalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, mask_paths=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform or RetinalAugmentationPipeline()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        \n",
    "        if self.mask_paths is not None:\n",
    "            mask = Image.open(self.mask_paths[idx])\n",
    "            views, transformed_masks = self.transform(image, mask)\n",
    "            return views, transformed_masks\n",
    "        else:\n",
    "            views = self.transform(image)\n",
    "            return views\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'b', 'd', 'c', 'a']\n",
      "[5, 2, 4, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "m = [\"a\" , \"b\"  ,\"c\" , \"d\" , \"e\"]\n",
    "\n",
    "pa = list(zip(m , l))\n",
    "import random\n",
    "\n",
    "random.shuffle(pa)\n",
    "\n",
    "m , l = map(list , zip(*pa))\n",
    "\n",
    "print(m)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, E, H', W')\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.MultiheadAttention(dim, heads, dropout=dropout),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, mlp_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(mlp_dim, dim),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for norm1, attn, norm2, mlp in self.layers:\n",
    "            x_norm = norm1(x)\n",
    "            attn_out, _ = attn(x_norm, x_norm, x_norm)\n",
    "            x = x + attn_out\n",
    "            x = x + mlp(norm2(x))\n",
    "        return x\n",
    "\n",
    "class IJEPA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        predictor_depth=4,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
    "        \n",
    "        # Target encoder\n",
    "        self.target_encoder = TransformerEncoder(\n",
    "            embed_dim,\n",
    "            encoder_depth,\n",
    "            num_heads,\n",
    "            embed_dim * mlp_ratio,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        # Context encoder\n",
    "        self.context_encoder = TransformerEncoder(\n",
    "            embed_dim,\n",
    "            encoder_depth,\n",
    "            num_heads,\n",
    "            embed_dim * mlp_ratio,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        # Predictor\n",
    "        self.predictor = TransformerEncoder(\n",
    "            embed_dim,\n",
    "            predictor_depth,\n",
    "            num_heads,\n",
    "            embed_dim * mlp_ratio,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def momentum_update(self, target_encoder, context_encoder, momentum=0.999):\n",
    "        \"\"\"Momentum update for target encoder\"\"\"\n",
    "        for target_params, context_params in zip(\n",
    "            target_encoder.parameters(), context_encoder.parameters()\n",
    "        ):\n",
    "            target_params.data = momentum * target_params.data + (1 - momentum) * context_params.data\n",
    "\n",
    "    def get_random_boxes(self, batch_size, n_boxes=4):\n",
    "        \"\"\"Generate random target boxes for masked prediction\"\"\"\n",
    "        boxes = []\n",
    "        for _ in range(batch_size):\n",
    "            batch_boxes = []\n",
    "            for _ in range(n_boxes):\n",
    "                x1 = torch.randint(0, 14, (1,)).item()\n",
    "                y1 = torch.randint(0, 14, (1,)).item()\n",
    "                w = torch.randint(2, 6, (1,)).item()\n",
    "                h = torch.randint(2, 6, (1,)).item()\n",
    "                batch_boxes.append([x1, y1, x1 + w, y1 + h])\n",
    "            boxes.append(batch_boxes)\n",
    "        return torch.tensor(boxes)\n",
    "\n",
    "    def extract_targets(self, features, boxes):\n",
    "        \"\"\"Extract target features based on boxes\"\"\"\n",
    "        B, N, D = features.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        features = rearrange(features, 'b (h w) d -> b d h w', h=H)\n",
    "        \n",
    "        target_features = []\n",
    "        for b in range(B):\n",
    "            batch_targets = []\n",
    "            for box in boxes[b]:\n",
    "                x1, y1, x2, y2 = box\n",
    "                target = features[b:b+1, :, y1:y2, x1:x2]\n",
    "                target = F.adaptive_avg_pool2d(target, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "                batch_targets.append(target)\n",
    "            target_features.append(torch.cat(batch_targets, dim=0))\n",
    "        return torch.stack(target_features)\n",
    "\n",
    "    def forward(self, images, boxes=None):\n",
    "        B = images.shape[0]\n",
    "        if boxes is None:\n",
    "            boxes = self.get_random_boxes(B)\n",
    "            \n",
    "        # Get patch embeddings\n",
    "        x = self.patch_embed(images)\n",
    "        \n",
    "        # Get context features\n",
    "        context_features = self.context_encoder(x)\n",
    "        \n",
    "        # Get target features (with no gradient)\n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_encoder(x)\n",
    "            target_features = self.extract_targets(target_features, boxes)\n",
    "        \n",
    "        # Predict target features\n",
    "        predicted_features = self.predictor(context_features)\n",
    "        predicted_features = self.extract_targets(predicted_features, boxes)\n",
    "        \n",
    "        return predicted_features, target_features\n",
    "\n",
    "class IJEPALoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, predicted_features, target_features):\n",
    "        # Normalize features\n",
    "        predicted_features = F.normalize(predicted_features, dim=-1)\n",
    "        target_features = F.normalize(target_features, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        sim = torch.einsum('bnd,bnd->bn', predicted_features, target_features)\n",
    "        \n",
    "        # Compute loss (negative cosine similarity)\n",
    "        loss = -sim.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def create_ijepa_model(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    embed_dim=768,\n",
    "    encoder_depth=12,\n",
    "    predictor_depth=4,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1\n",
    "):\n",
    "    model = IJEPA(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=embed_dim,\n",
    "        encoder_depth=encoder_depth,\n",
    "        predictor_depth=predictor_depth,\n",
    "        num_heads=num_heads,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    criterion = IJEPALoss()\n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR specific IJEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=2048, patch_size=32, in_chans=3, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Hierarchical patch embedding to handle large images\n",
    "        self.hierarchical_proj = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim//4, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LayerNorm([embed_dim//4, img_size//2, img_size//2]),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim//4, embed_dim//2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LayerNorm([embed_dim//2, img_size//4, img_size//4]),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(embed_dim//2, embed_dim, kernel_size=patch_size//4, stride=patch_size//4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hierarchical_proj(x)  # (B, E, H', W')\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.MultiheadAttention(dim, heads, dropout=dropout),\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, mlp_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(mlp_dim, dim),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for norm1, attn, norm2, mlp in self.layers:\n",
    "            x_norm = norm1(x)\n",
    "            attn_out, _ = attn(x_norm, x_norm, x_norm)\n",
    "            x = x + attn_out\n",
    "            x = x + mlp(norm2(x))\n",
    "        return x\n",
    "\n",
    "class MedicalIJEPA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=2048,\n",
    "        patch_size=32,  # Larger patch size for memory efficiency\n",
    "        in_chans=3,\n",
    "        embed_dim=1024,  # Increased embedding dimension\n",
    "        encoder_depth=12,\n",
    "        predictor_depth=4,\n",
    "        num_heads=16,  # Increased number of heads\n",
    "        mlp_ratio=4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
    "        \n",
    "        # Target encoder\n",
    "        self.target_encoder = TransformerEncoder(\n",
    "            embed_dim,\n",
    "            encoder_depth,\n",
    "            num_heads,\n",
    "            embed_dim * mlp_ratio,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        # Context encoder\n",
    "        self.context_encoder = TransformerEncoder(\n",
    "            embed_dim,\n",
    "            encoder_depth,\n",
    "            num_heads,\n",
    "            embed_dim * mlp_ratio,\n",
    "            dropout\n",
    "        )\n",
    "        \n",
    "        # Predictor with medical-specific adjustments\n",
    "        self.predictor = nn.Sequential(\n",
    "            TransformerEncoder(\n",
    "                embed_dim,\n",
    "                predictor_depth,\n",
    "                num_heads,\n",
    "                embed_dim * mlp_ratio,\n",
    "                dropout\n",
    "            ),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim)  # Additional projection layer\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Save grid size for box generation\n",
    "        self.grid_size = img_size // patch_size\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_random_boxes(self, batch_size, n_boxes=6):  # Increased number of boxes\n",
    "        \"\"\"Generate random target boxes with medical imaging considerations\"\"\"\n",
    "        boxes = []\n",
    "        for _ in range(batch_size):\n",
    "            batch_boxes = []\n",
    "            # Ensure better coverage of the retinal area\n",
    "            for _ in range(n_boxes):\n",
    "                # Generate boxes more likely to be in the central region\n",
    "                center_bias = torch.randn(2) * 0.5\n",
    "                x_center = self.grid_size // 2 + int(center_bias[0] * self.grid_size // 4)\n",
    "                y_center = self.grid_size // 2 + int(center_bias[1] * self.grid_size // 4)\n",
    "                \n",
    "                # Random box size (larger for better feature capture)\n",
    "                w = torch.randint(4, 8, (1,)).item()\n",
    "                h = torch.randint(4, 8, (1,)).item()\n",
    "                \n",
    "                x1 = max(0, x_center - w // 2)\n",
    "                y1 = max(0, y_center - h // 2)\n",
    "                x2 = min(self.grid_size, x1 + w)\n",
    "                y2 = min(self.grid_size, y1 + h)\n",
    "                \n",
    "                batch_boxes.append([x1, y1, x2, y2])\n",
    "            boxes.append(batch_boxes)\n",
    "        return torch.tensor(boxes)\n",
    "\n",
    "    def forward(self, images, boxes=None):\n",
    "        B = images.shape[0]\n",
    "        if boxes is None:\n",
    "            boxes = self.get_random_boxes(B)\n",
    "            \n",
    "        # Get patch embeddings\n",
    "        x = self.patch_embed(images)\n",
    "        \n",
    "        # Get context features\n",
    "        context_features = self.context_encoder(x)\n",
    "        \n",
    "        # Get target features (with no gradient)\n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_encoder(x)\n",
    "            target_features = self.extract_targets(target_features, boxes)\n",
    "        \n",
    "        # Predict target features\n",
    "        predicted_features = self.predictor(context_features)\n",
    "        predicted_features = self.extract_targets(predicted_features, boxes)\n",
    "        \n",
    "        return predicted_features, target_features\n",
    "\n",
    "    def extract_targets(self, features, boxes):\n",
    "        \"\"\"Extract target features with medical imaging considerations\"\"\"\n",
    "        B, N, D = features.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        features = rearrange(features, 'b (h w) d -> b d h w', h=H)\n",
    "        \n",
    "        target_features = []\n",
    "        for b in range(B):\n",
    "            batch_targets = []\n",
    "            for box in boxes[b]:\n",
    "                x1, y1, x2, y2 = box\n",
    "                target = features[b:b+1, :, y1:y2, x1:x2]\n",
    "                # Use adaptive pooling for variable size boxes\n",
    "                target = F.adaptive_avg_pool2d(target, (2, 2))\n",
    "                target = rearrange(target, 'b c h w -> b (h w) c')\n",
    "                batch_targets.append(target)\n",
    "            target_features.append(torch.cat(batch_targets, dim=1))\n",
    "        return torch.stack(target_features)\n",
    "\n",
    "def create_medical_ijepa(\n",
    "    img_size=2048,\n",
    "    patch_size=32,\n",
    "    in_chans=3,\n",
    "    embed_dim=1024,\n",
    "    encoder_depth=12,\n",
    "    predictor_depth=4,\n",
    "    num_heads=16,\n",
    "    mlp_ratio=4,\n",
    "    dropout=0.1\n",
    "):\n",
    "    model = MedicalIJEPA(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=embed_dim,\n",
    "        encoder_depth=encoder_depth,\n",
    "        predictor_depth=predictor_depth,\n",
    "        num_heads=num_heads,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    criterion = nn.MSELoss()  # Using MSE loss for medical imaging\n",
    "    return model, criterion\n",
    "\n",
    "# Medical image specific transforms\n",
    "def get_medical_transforms(img_size=2048):\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR specific IJEPA with adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "\n",
    "class DRSpecificAugmentation:\n",
    "    def __init__(self):\n",
    "        self.transform = A.Compose([\n",
    "            # DR-specific augmentations\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1, \n",
    "                contrast_limit=0.1, \n",
    "                p=0.5\n",
    "            ),\n",
    "            A.OneOf([\n",
    "                # Simulate different lighting conditions\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "                A.MultiplicativeNoise(multiplier=(0.95, 1.05), p=0.5),\n",
    "            ], p=0.3),\n",
    "            A.OneOf([\n",
    "                # Simulate different imaging conditions\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "                A.MedianBlur(blur_limit=3, p=0.3),\n",
    "            ], p=0.2),\n",
    "            # Lesion-focused augmentations\n",
    "            A.OneOf([\n",
    "                A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5),\n",
    "                A.UnsharpMask(blur_limit=(3, 7), p=0.5),\n",
    "            ], p=0.3),\n",
    "            # Color augmentations for different camera settings\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=5,\n",
    "                sat_shift_limit=10,\n",
    "                val_shift_limit=5,\n",
    "                p=0.3\n",
    "            ),\n",
    "            # Preserve circular FOV\n",
    "            A.CoarseDropout(\n",
    "                max_holes=8,\n",
    "                max_height=32,\n",
    "                max_width=32,\n",
    "                min_holes=1,\n",
    "                min_height=8,\n",
    "                min_width=8,\n",
    "                fill_value=0,\n",
    "                mask_fill_value=0,\n",
    "                p=0.2\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image=image)['image']\n",
    "\n",
    "class MemoryEfficientBatchSampler:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.gradient_accumulation_steps = 4  # Adjust based on memory constraints\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = torch.randperm(len(self.dataset))\n",
    "        else:\n",
    "            indices = torch.arange(len(self.dataset))\n",
    "            \n",
    "        # Create mini-batches\n",
    "        mini_batch_size = self.batch_size // self.gradient_accumulation_steps\n",
    "        for i in range(0, len(indices), mini_batch_size):\n",
    "            yield indices[i:i + mini_batch_size]\n",
    "\n",
    "class DRLesionAttention(nn.Module):\n",
    "    \"\"\"Attention module specifically designed for DR lesions\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.lesion_queries = nn.Parameter(torch.randn(1, 5, dim))  # 5 different lesion types\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, n, d = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n d -> b n d'), qkv)\n",
    "        \n",
    "        # Add lesion-specific queries\n",
    "        lesion_queries = self.lesion_queries.repeat(b, 1, 1)\n",
    "        q = torch.cat([q, lesion_queries], dim=1)\n",
    "        \n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class DRSpecificIJEPA(MedicalIJEPA):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Add DR-specific components\n",
    "        self.lesion_attention = DRLesionAttention(kwargs.get('embed_dim', 1024))\n",
    "        \n",
    "        # Add lesion-specific prediction heads\n",
    "        self.lesion_heads = nn.ModuleDict({\n",
    "            'microaneurysms': nn.Linear(kwargs.get('embed_dim', 1024), 1),\n",
    "            'hemorrhages': nn.Linear(kwargs.get('embed_dim', 1024), 1),\n",
    "            'hard_exudates': nn.Linear(kwargs.get('embed_dim', 1024), 1),\n",
    "            'cotton_wool_spots': nn.Linear(kwargs.get('embed_dim', 1024), 1),\n",
    "            'neovascularization': nn.Linear(kwargs.get('embed_dim', 1024), 1)\n",
    "        })\n",
    "\n",
    "    def forward(self, images, boxes=None):\n",
    "        features, target_features = super().forward(images, boxes)\n",
    "        \n",
    "        # Apply lesion attention\n",
    "        features_with_attention = self.lesion_attention(features)\n",
    "        \n",
    "        # Get lesion-specific predictions\n",
    "        lesion_predictions = {\n",
    "            name: head(features_with_attention.mean(1))\n",
    "            for name, head in self.lesion_heads.items()\n",
    "        }\n",
    "        \n",
    "        return features, target_features, lesion_predictions\n",
    "\n",
    "class DRTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        grad_accumulation_steps=4,\n",
    "        mixed_precision=True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.grad_accumulation_steps = grad_accumulation_steps\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.scaler = GradScaler() if mixed_precision else None\n",
    "        \n",
    "    def train_step(self, batch, lesion_labels=None):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Split batch for gradient accumulation\n",
    "        micro_batches = torch.chunk(batch, self.grad_accumulation_steps)\n",
    "        \n",
    "        for i, micro_batch in enumerate(micro_batches):\n",
    "            with autocast(enabled=self.mixed_precision):\n",
    "                features, target_features, lesion_preds = self.model(micro_batch)\n",
    "                \n",
    "                # Compute main I-JEPA loss\n",
    "                main_loss = self.criterion(features, target_features)\n",
    "                \n",
    "                # Compute lesion prediction loss if labels are provided\n",
    "                lesion_loss = 0\n",
    "                if lesion_labels is not None:\n",
    "                    for name, pred in lesion_preds.items():\n",
    "                        lesion_loss += F.binary_cross_entropy_with_logits(\n",
    "                            pred,\n",
    "                            lesion_labels[name][i * len(micro_batch):(i + 1) * len(micro_batch)]\n",
    "                        )\n",
    "                \n",
    "                loss = main_loss + 0.5 * lesion_loss\n",
    "                scaled_loss = loss / self.grad_accumulation_steps\n",
    "                \n",
    "            if self.mixed_precision:\n",
    "                self.scaler.scale(scaled_loss).backward()\n",
    "            else:\n",
    "                scaled_loss.backward()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Update weights\n",
    "        if self.mixed_precision:\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        return total_loss / self.grad_accumulation_steps\n",
    "\n",
    "def create_dr_specific_transforms(img_size=2048):\n",
    "    \"\"\"Create DR-specific image transformations\"\"\"\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        DRSpecificAugmentation(),\n",
    "    ])\n",
    "    \n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "def train_dr_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4\n",
    "):\n",
    "    \"\"\"Training loop with DR-specific optimizations\"\"\"\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs\n",
    "    )\n",
    "    \n",
    "    trainer = DRTrainer(\n",
    "        model=model,\n",
    "        criterion=nn.MSELoss(),\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        mixed_precision=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch, lesion_labels in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            loss = trainer.train_step(batch, lesion_labels)\n",
    "            train_loss += loss\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch, lesion_labels in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                features, target_features, lesion_preds = model(batch)\n",
    "                loss = trainer.criterion(features, target_features)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
